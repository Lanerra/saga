# drafting_agent.py
import logging
from typing import Tuple, Optional, List, Dict, Any

import config
import llm_interface
from type import SceneDetail
from prompt_data_getters import (
    get_filtered_character_profiles_for_prompt_plain_text,
    get_filtered_world_data_for_prompt_plain_text,
)
# Hybrid context is generated by orchestrator or context agent and passed in.

logger = logging.getLogger(__name__)

class DraftingAgent:
    def __init__(self, model_name: str = config.DRAFTING_MODEL):
        self.model_name = model_name
        logger.info(f"DraftingAgent initialized with model: {self.model_name}")

    def _format_scene_plan_for_prompt(self, chapter_plan: List[SceneDetail], model_name_for_tokens: str, max_tokens_budget: int) -> str:
        """
        Formats the chapter plan (list of SceneDetail dicts) into plain text for the LLM prompt, respecting token limits.
        (Logic from chapter_drafting_logic.py _format_scene_plan_for_prompt function)
        """
        if not chapter_plan:
            return "No detailed scene plan available."

        plan_text_lines = ["**Detailed Scene Plan (MUST BE FOLLOWED CLOSELY):**"]
        # total_plan_text_so_far = "\n".join(plan_text_lines) + "\n" # Initial calculation before loop

        # Build the plan string incrementally
        current_plan_str_parts = [plan_text_lines[0]] # Start with header

        for scene_idx, scene in enumerate(chapter_plan):
            scene_lines_parts = [
                f"Scene Number: {scene.get('scene_number', 'N/A')}",
                f"  Summary: {scene.get('summary', 'N/A')}",
                f"  Characters Involved: {', '.join(scene.get('characters_involved', [])) if scene.get('characters_involved') else 'None'}",
                "  Key Dialogue Points:"
            ]
            for point in scene.get('key_dialogue_points', []):
                scene_lines_parts.append(f"    - {point}")
            scene_lines_parts.append(f"  Setting Details: {scene.get('setting_details', 'N/A')}")
            scene_lines_parts.append("  Scene Focus Elements:")
            for focus_el in scene.get('scene_focus_elements', []):
                scene_lines_parts.append(f"    - {focus_el}")
            scene_lines_parts.append(f"  Contribution: {scene.get('contribution', 'N/A')}")
            
            if scene_idx < len(chapter_plan) -1 :
                scene_lines_parts.append("-" * 20)
            
            # Check token count before adding this scene's text to the cumulative plan
            prospective_next_segment = "\n".join(scene_lines_parts)
            # Test adding the new segment to the current plan parts
            temp_full_plan_text = "\n".join(current_plan_str_parts + [prospective_next_segment])
            
            if llm_interface.count_tokens(temp_full_plan_text, model_name_for_tokens) > max_tokens_budget:
                current_plan_str_parts.append("... (plan truncated in prompt due to token limit)")
                logger.warning(f"Chapter plan was token-truncated for the drafting prompt. Max tokens for plan: {max_tokens_budget}. Stopped before scene {scene.get('scene_number', 'N/A')}.")
                break
            
            current_plan_str_parts.append(prospective_next_segment) # Add the segment if it fits
            
        if len(current_plan_str_parts) <= 1 : # Only header means no scenes were added
            return "No detailed scene plan available or plan was too long to include any scenes."
            
        return "\n".join(current_plan_str_parts)

    async def draft_chapter(
        self,
        agent_or_props: Any, 
        chapter_number: int, # This is the novel's actual chapter number
        plot_point_focus: Optional[str], # The specific plot point for this chapter
        hybrid_context: str,
        chapter_plan: Optional[List[SceneDetail]]
    ) -> Tuple[Optional[str], Optional[str], Optional[Dict[str, int]]]:
        """
        Generates the initial draft text for a chapter.
        Returns the draft, raw LLM output, and LLM usage data.
        'agent_or_props' can be the NANA_Orchestrator instance or a novel_props dictionary.
        """
        if not plot_point_focus:
            plot_point_focus = "Continue the narrative logically, focusing on character development and plot progression based on previous events."
            logger.warning(f"Plot point focus was None for Ch {chapter_number} draft generation. Using generic fallback.")

        plan_section_for_prompt_parts: List[str] = []
        if config.ENABLE_AGENTIC_PLANNING:
            if chapter_plan and isinstance(chapter_plan, list):
                max_plan_tokens_for_prompt = config.MAX_CONTEXT_TOKENS // 3
                plan_section_for_prompt_parts.append(self._format_scene_plan_for_prompt(chapter_plan, self.model_name, max_plan_tokens_for_prompt))
                logger.info(f"Using detailed scene plan (plain text) for Ch {chapter_number} draft generation.")
            else:
                plan_section_for_prompt_parts.append(f"**Chapter Plan Note:** No detailed scene plan available. Rely on the Overall Plot Point Focus.\n**Overall Plot Point Focus for THIS Chapter:** {plot_point_focus}\n")
        else:
            plan_section_for_prompt_parts.append(f"**Chapter Plan Note:** Detailed agentic planning is disabled. Rely on the Overall Plot Point Focus.\n**Overall Plot Point Focus for THIS Chapter:** {plot_point_focus}\n")
        
        plan_section_for_prompt_str = "".join(plan_section_for_prompt_parts)

        char_profiles_plain_text = await get_filtered_character_profiles_for_prompt_plain_text(agent_or_props, chapter_number - 1)
        world_building_plain_text = await get_filtered_world_data_for_prompt_plain_text(agent_or_props, chapter_number - 1)

        plot_outline_data: Dict[str, Any] = {}
        all_plot_points: List[str] = []
        plot_point_index = -1 # Default if not found

        if isinstance(agent_or_props, dict): 
            plot_outline_data = agent_or_props.get('plot_outline_full', agent_or_props.get('plot_outline', {}))
        elif hasattr(agent_or_props, 'plot_outline'):
            plot_outline_data = agent_or_props.plot_outline
        else:
            logger.warning("Could not determine plot_outline_data source in DraftingAgent.")
        
        all_plot_points = plot_outline_data.get('plot_points', [])
        if chapter_number > 0 and chapter_number <= len(all_plot_points):
            plot_point_index = chapter_number - 1
        
        total_plot_points_in_novel = len(all_plot_points)

        future_plot_context_parts: List[str] = []
        if plot_point_index >= 0 and plot_point_index + 1 < total_plot_points_in_novel:
            next_pp_text = all_plot_points[plot_point_index + 1]
            if isinstance(next_pp_text, str) and next_pp_text.strip():
                future_plot_context_parts.append(f"\n**Anticipated Next Major Plot Point (PP {plot_point_index + 2}/{total_plot_points_in_novel} - for context, do not address this yet):**\n{next_pp_text.strip()}")
            if plot_point_index + 2 < total_plot_points_in_novel:
                 next_next_pp_text = all_plot_points[plot_point_index + 2]
                 if isinstance(next_next_pp_text, str) and next_next_pp_text.strip():
                    future_plot_context_parts.append(f"\n**And Then (PP {plot_point_index + 3}/{total_plot_points_in_novel} - distant context):**\n{next_next_pp_text.strip()}")
        future_plot_context_str = "".join(future_plot_context_parts)

        prompt_lines = [
            "/no_think",
            f"You are an expert novelist tasked with writing Chapter {chapter_number} of the novel titled \"{plot_outline_data.get('title', 'Untitled Novel')}\".",
            "This chapter is part of a larger narrative arc.",
            "",
            "**Story Bible / Core Information:**",
            f"  - Genre: {plot_outline_data.get('genre', 'N/A')}",
            f"  - Central Theme: {plot_outline_data.get('theme', 'N/A')}",
            f"  - Protagonist: {plot_outline_data.get('protagonist_name', 'N/A')}",
            f"  - Protagonist's Character Arc: {plot_outline_data.get('character_arc', 'N/A')}",
            "",
            f"**Overall Narrative Context:** This chapter focuses on Plot Point {plot_point_index + 1} of {total_plot_points_in_novel} total major plot points in the novel.",
            future_plot_context_str,
            "",
            plan_section_for_prompt_str,
            "",
            "**World Building Notes (Plain Text - pay attention to any 'prompt_notes' indicating provisional data):**",
            "```text",
            world_building_plain_text if world_building_plain_text.strip() else "No specific world building notes provided for this chapter's context.",
            "```",
            "**Character Profiles (Plain Text - pay attention to any 'prompt_notes' indicating provisional status):**",
            "```text",
            char_profiles_plain_text if char_profiles_plain_text.strip() else "No specific character profiles provided for this chapter's context.",
            "```",
            "**Hybrid Context (Semantic Context for Flow & KG Facts for Canon - from previous chapters):**",
            "--- BEGIN HYBRID CONTEXT ---",
            hybrid_context if hybrid_context.strip() else "No previous context (e.g., this is Chapter 1 or context retrieval failed).",
            "--- END HYBRID CONTEXT ---",
            "",
            "**Writing Instructions:**",
            f"1. Write a compelling chapter of at least {config.MIN_ACCEPTABLE_DRAFT_LENGTH} characters.",
            "2. Primary Goal: Advance this chapter's **Overall Plot Point Focus**. Avoid resolving future plot points from 'Overall Narrative Context'.",
            "3. Scene Plan Adherence: If a **Detailed Scene Plan** is given, follow it meticulously, especially its 'Scene Focus Elements' to add depth and length.",
            "4. Consistency: Maintain consistency with Story Bible, World Building, Character Profiles, and Previous Context.",
            "   - KG Facts: `KEY RELIABLE KG FACTS` in `HYBRID CONTEXT` are **canon and MUST be respected**.",
            "   - Semantic Context: Use `SEMANTIC CONTEXT` in `HYBRID CONTEXT` for narrative flow, tone, and recent event recall.",
            f"5. Style: Ensure smooth narrative flow and vivid prose for the '{plot_outline_data.get('genre', 'story')}' genre.",
            "6. Show, Don't Tell: Use vivid descriptions, sensory details, actions, internal thoughts, and dialogue to convey information and emotion.",
            "7. Dialogue: Develop natural dialogue with subtext, emotion, and non-verbal cues. Longer conversations are acceptable if they serve purpose.",
            "8. Internal Reactions: Thoroughly explore key characters' thoughts and feelings regarding events.",
            "9. **CRITICAL: Avoid Redundancy and Repetition.** Do NOT repeat narrative beats, descriptions, specific phrasing, or dialogue patterns that have already been established in the 'Hybrid Context' or earlier in *this current chapter's generation*. Each scene and paragraph should introduce *new information, development, or a fresh perspective* on existing elements. Ensure forward momentum in the plot and character arcs.",
            "10. Output: **ONLY the chapter text.** No headers, titles, or meta-discussion.",
            "",
            f"--- BEGIN CHAPTER {chapter_number} TEXT ---"
        ]
        prompt = "\n".join(prompt_lines)

        logger.info(f"Calling LLM ({self.model_name}) for Ch {chapter_number} draft. Plot Point {plot_point_index+1}/{total_plot_points_in_novel}. Target min length: {config.MIN_ACCEPTABLE_DRAFT_LENGTH} chars.")
        raw_llm_text, usage_data = await llm_interface.async_call_llm(
            model_name=self.model_name,
            prompt=prompt,
            temperature=config.TEMPERATURE_DRAFTING,
            max_tokens=None, 
            allow_fallback=True,
            stream_to_disk=True,
            frequency_penalty=config.FREQUENCY_PENALTY_DRAFTING,
            presence_penalty=config.PRESENCE_PENALTY_DRAFTING
        )
        if not raw_llm_text:
            logger.error(f"LLM returned no content for Ch {chapter_number} draft (primary and potential fallback failed).")
            return None, None, usage_data

        cleaned_text = llm_interface.clean_model_response(raw_llm_text)

        if not cleaned_text or len(cleaned_text) < 50:
            logger.error(f"Ch {chapter_number} draft has virtually no content after cleaning ({len(cleaned_text or '')} chars). Raw LLM output snippet: '{raw_llm_text[:200]}...'")
            return None, raw_llm_text, usage_data

        if len(cleaned_text) < config.MIN_ACCEPTABLE_DRAFT_LENGTH:
             logger.warning(
                 f"Ch {chapter_number} draft is short ({len(cleaned_text)} chars) after cleaning, but will be passed for evaluation/revision. "
                 f"Min required: {config.MIN_ACCEPTABLE_DRAFT_LENGTH}. "
                 f"Snippet: '{cleaned_text[:200].replace(chr(10), ' ')}...'"
             )

        logger.info(f"Generated initial draft for ch {chapter_number} (Length: {len(cleaned_text)} chars).")
        return cleaned_text, raw_llm_text, usage_data