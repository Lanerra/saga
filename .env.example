# .env
# API and Model Configuration
OLLAMA_EMBED_URL=http://127.0.0.1:11434
OPENAI_API_BASE=http://127.0.0.1:8080/v1
OPENAI_API_KEY=nope
EMBEDDING_MODEL=ZimaBlueAI/Qwen3-Embedding-0.6B:Q8_0
EXPECTED_EMBEDDING_DIM=1024
ENABLE_RERANKING=True

# Neo4j Connection Settings
NEO4J_URI=bolt://localhost:7687
NEO4J_USER=neo4j
NEO4J_PASSWORD=saga_password
NEO4J_DATABASE=neo4j
NEO4J_VECTOR_DIMENSIONS=1024

# Model Aliases
LARGE_MODEL=Qwen3-8B-Q4
MEDIUM_MODEL=Qwen3-8B-Q4
SMALL_MODEL=Qwen3-4B-Q4
NARRATOR_MODEL=Sentinel-Serpent-Q3-32B-Q4
INITIAL_SETUP_MODEL=Sentinel-Serpent-Q3-32B-Q4

# Task-specific Temperatures
# Creative tasks: Higher temperature for more varied output
TEMPERATURE_INITIAL_SETUP=0.8
TEMPERATURE_DRAFTING=0.8
TEMPERATURE_REVISION=0.65
TEMPERATURE_PLANNING=0.6

# Analytical/Extraction tasks: Lower temperature for more deterministic and factual output
TEMPERATURE_EVALUATION=0.3
TEMPERATURE_CONSISTENCY_CHECK=0.2
TEMPERATURE_KG_EXTRACTION=0.4
TEMPERATURE_SUMMARY=0.5
TEMPERATURE_PATCH=0.7


# --- SAGA Generation Parameters ---
# Toggle for the "/no_think" directive in LLM prompts.
ENABLE_LLM_NO_THINK_DIRECTIVE=True

# Novel Configuration
# Enable unhinged plot mode (True/False)
UNHINGED_PLOT_MODE=False

# Generation Run Settings
# Number of total plot points in the whole narrative
TARGET_PLOT_POINTS_INITIAL_GENERATION=18

# Maximum chapters to generate per run; loop stops early if no plot points remain
CHAPTERS_PER_RUN=6

# Scene Planning (Agentic Planning)
# Minimum number of scenes to target per chapter plan
TARGET_SCENES_MIN=4
# Maximum number of scenes to target per chapter plan
TARGET_SCENES_MAX=6

# Token Limits
# Maximum context tokens for LLM calls
MAX_CONTEXT_TOKENS=40960
# Maximum tokens for LLM generation
MAX_GENERATION_TOKENS=32768
# Maximum tokens for summary generation
MAX_SUMMARY_TOKENS=4096
# Maximum tokens for knowledge graph triple extraction
MAX_KG_TRIPLE_TOKENS=8192
# Maximum tokens for knowledge graph pre-population
MAX_PREPOP_KG_TOKENS=16384
# Maximum tokens for planning phase
MAX_PLANNING_TOKENS=16384

# Draft Quality & Length
# Minimum acceptable character length for a draft chapter.
# WARNING: Setting this significantly higher than 13000 (e.g., 15000+)
# can lead to AI generating excessively long chapters (30,000+ characters)
# as it tries to meet the length requirement, potentially sacrificing quality or coherence.
MIN_ACCEPTABLE_DRAFT_LENGTH=12000

# Revision Process
# Enable patch-based revision system (True/False)
ENABLE_PATCH_BASED_REVISION=True
# Enable or disable validation of patch instructions (True/False)
AGENT_ENABLE_PATCH_VALIDATION=True
# Maximum number of patch instructions to generate per revision cycle
MAX_PATCH_INSTRUCTIONS_TO_GENERATE=10
# Coherence threshold for revision (e.g., 0.60)
REVISION_COHERENCE_THRESHOLD=0.60
# Similarity acceptance threshold (if patched text is this similar to original, log warning)
REVISION_SIMILARITY_ACCEPTANCE=0.995
# Allowed remaining problems after patching before forcing a full rewrite
POST_PATCH_PROBLEM_THRESHOLD=2

# LLM Call Robustness
# Number of retry attempts for LLM calls
LLM_RETRY_ATTEMPTS=3

# --- Logging ---
# Log level (DEBUG, INFO, WARNING, ERROR, CRITICAL)
LOG_LEVEL=INFO # Example if you want to override default INFO

# --- Rich Progress Display ---
# Enable Rich progress display (True/False)
ENABLE_RICH_PROGRESS=True # Example
