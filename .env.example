# .env
# Core API and Connection Configuration
# -----------------------------------------------------------------
# Ensure these point to your running LLM and Neo4j services.
OPENAI_API_BASE="http://127.0.0.1:8080/v1"
OPENAI_API_KEY="sk-nope"
EMBEDDING_API_BASE="http://127.0.0.1:11434"
EMBEDDING_API_KEY=""
NEO4J_URI="bolt://localhost:7687"
NEO4J_USER="neo4j"
NEO4J_PASSWORD="saga_password"

# -----------------------------------------------------------------
# Model Assignments (Tiered for Optimal Performance & Quality)
# -----------------------------------------------------------------
# Base models available on your server.
LARGE_MODEL="gemma3-12b"
MEDIUM_MODEL="llama3.3-8b"
SMALL_MODEL="qwen3-4b"
NARRATIVE_MODEL="gemma3-12b"

EMBEDDING_MODEL="nomic-embed-text:latest"
EXPECTED_EMBEDDING_DIM=768
NEO4J_VECTOR_DIMENSIONS=768

# -----------------------------------------------------------------
# Generation & Revision Parameters
# -----------------------------------------------------------------
CHAPTERS_PER_RUN=5                       # A good batch size for a single run
TARGET_PLOT_POINTS_INITIAL_GENERATION=12 # Number of plot points in the story
MAX_CONTEXT_TOKENS=32768                 # Maximum context tokens for LLM calls
MAX_GENERATION_TOKENS=16384              # Maximum tokens for LLM generation
TARGET_SCENES_MIN=3                      # Minimum number of scenes to target per chapter plan
TARGET_SCENES_MAX=5                      # Maximum number of scenes to target per chapter plan

# -----------------------------------------------------------------
# Logging Configuration
# -----------------------------------------------------------------
LOG_LEVEL="INFO" # Use "DEBUG" for more verbose output, "INFO" for standard runs.
